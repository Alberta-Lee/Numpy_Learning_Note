{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy 与 神经网络\n",
    "\n",
    "对我来说用于分类的神经网络是一种非常可怕的机器学习算法。学习神经网络算法时，会给人一种望而生畏的感觉，但当我最终妥协并陷入其中无法自拔的时候，才发现其实它并没有想象中的那么可怕。它们被称为神经网络，是因为它们松散地建立在人类大脑神经元以及神经元工作原理的基础上。但是，它们本质上是一组线性模型。关于这些算法的数学和结构有很多很好的文章来解释它们，所以这些部分我的这篇文章不会提及。相反，我将详细的用numpy库在python中编写一个一个的步骤，并非常清楚地解释它的。这篇文章的代码很大程度上基于[《集体智慧编程》](https://s.click.taobao.com/t?e=m%3D2%26s%3DXIetsYhTCu8cQipKwQzePOeEDrYVVa64K7Vc7tFgwiHjf2vlNIV67pZpQLiTO%2BhgmSMhGfkQJ77VdTmGfLKGc3msngnYL0uHYhNjQr6GXJQ0IVmWuK%2BMt0g0aHp6CeiC6hqtRuAxoUJbnlHS8Kikd9qH4uMbv1iQxgxdTc00KD8%3D&pvid=10_183.14.30.247_9333_1539405668948)中提供的神经网络代码，只要输入数据格式正确，我就稍微调整它以使其可用于任何数据集。\n",
    "\n",
    "首先，我们可以将每个神经元视为具有激活功能。此功能确定神经元是 ``开`` 还是 ``关`` - 是否激活。我们将使用sigmoid函数，在逻辑回归中，它应该是非常见的函数。与逻辑回归不同，我们在使用神经网络时也需要sigmoid函数的导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# derivative of sigmoid\n",
    "# sigmoid(y) * (1.0 - sigmoid(y))\n",
    "# the way we use this y is already sigmoided\n",
    "def dsigmoid(y):\n",
    "    return y * (1.0 - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就像逻辑回归一样，神经网络中的Sigmoid函数将生成输入的端点(激活)乘以它们的权重。例如，假设我们有两列(特征)的输入数据和一个隐藏节点(神经元)在我们的神经网络。每个特征都会乘以相应的权重值，然后相加，然后通过S形(就像逻辑回归一样)。以这个简单的例子，并把它变成一个神经网络，我们只是添加更多的隐藏单元。除了添加更多的隐藏单元外，我们还将每个输入特性的路径添加到每个隐藏单元，并将其乘以相应的权重。每个隐藏单元取其输入*权值之和，并通过S形传递，从而导致该单元的激活。\n",
    "\n",
    "接下来，我们将设置数组来保存用于网络的数据，并初始化一些参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MLP_NeuralNetwork(object):\n",
    "    def __init__(self,input,hidden,output):\n",
    "        \"\"\"\n",
    "        :param input: number of input neurons\n",
    "        :param hidden: number of hidden neurons\n",
    "        :param output: number of output neurons\n",
    "        \"\"\"\n",
    "        self.input = input + 1 # add 1 for bias node\n",
    "        self.hidden = hidden\n",
    "        self.output = output\n",
    "        \n",
    "        # set up array of 1s for activations\n",
    "        self.ai = [1.0] * self.input\n",
    "        self.ah = [1.0] * self.hidden\n",
    "        self.ao = [1.0] * self.output\n",
    "        \n",
    "        # create randomized weights\n",
    "        self.wi = np.random.randn(self.input, self.hidden) \n",
    "        self.wo = np.random.randn(self.hidden, self.output) \n",
    "        \n",
    "        # create arrays of 0 for changes\n",
    "        self.ci = np.zeros((self.input, self.hidden))\n",
    "        self.co = np.zeros((self.hidden, self.output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们要用矩阵做所有这些计算，因为它们速度快，而且非常容易阅读。我们的类将接受三个输入：输入层的大小(特性)、隐藏层的大小(要调优的变量参数)和输出层的数量(可能的类的数量)。我们设置一个1数组作为单元激活的占位符，一个0数组作为层更改的占位符。需要注意的一件重要事情是，我们将所有的权重初始化为随机数。重要的是权值是随机的，否则我们将无法调整网络。如果所有的权重是一样的，那么所有隐藏的单位都是一样的，那你的神经网络算法就废了。\n",
    "\n",
    "所以现在是时候做一些预测的运算操作了。我们要做的是通过随机权重将所有数据通过网络提供给用户，并生成一些(不那么准确的)预测。后来，每次做出预测时，我们都会计算出预测的错误程度，以及为了使预测更好(即误差)，我们需要改变权重的方向。我们会做很多…很多次，当权重被更新时，我们会创建一个前馈函数，这个函数可以被一次又一次地调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedForward(self,inputs):\n",
    "    if len(inputs) != self.input - 1:\n",
    "        raise ValueError('Wrong number of inputs you silly goose!')\n",
    "        \n",
    "    # input activations\n",
    "    for i in range(self.input -1): # -1 is to avoid the bias\n",
    "        self.ai[i] = inputs[i]\n",
    "        \n",
    "    # hidden activations\n",
    "    for j in range(self.hidden):\n",
    "        sum = 0.0\n",
    "        for i in range(self.input):\n",
    "            sum += self.ai[i] * self.wi[i][j]\n",
    "        self.ah[j] = sigmoid(sum)\n",
    "        \n",
    "    # output activations\n",
    "    for k in range(self.output):\n",
    "        sum = 0.0\n",
    "        for j in range(self.hidden):\n",
    "            sum += self.ah[j] * self.wo[j][k]\n",
    "        self.ao[k] = sigmoid(sum)\n",
    "    return self.ao[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入激活只是输入功能。但是，对于另一层，激活变成了前一层激活的总和乘以它们的相应的权值，反馈到S形中去了。\n",
    "\n",
    "在第一次运算之后，我们的预测的误差相当大的。所以我们将使用一个非常熟悉的概念，梯度下降。这是我感到兴奋的部分，因为我认为数学真的很聪明。与线性模型的梯度下降不同，我们需要对神经网络使用一点微积分。这就是为什么我们在开始的时候，为S函数的导数写了这个函数。\n",
    "\n",
    "我们的反向传播算法首先计算我们预测的输出与真实输出的误差。然后我们在输出激活(预测值)上取S形的导数，以得到梯度的方向(斜率)，并将该值乘以误差。这就给了我们误差的大小，隐藏的权值需要改变哪个方向来修正它。然后我们进入到隐藏层，并根据前面计算的幅度和误差计算隐藏层权值的误差。\n",
    "\n",
    "利用该误差和隐藏层激活的S形导数，我们计算了输入层的权重需要改变多少，以及在哪个方向上需要改变。\n",
    "\n",
    "现在我们有了价值网络，我们想改变利率的多少，以及在什么方向上，我们真正做到了这一点。我们更新连接每一层的权重。我们通过将当前权重乘以学习速率常数以及相应的权重层的大小和方向来实现这一点。就像在线性模型中一样，我们使用学习速率常数在每一步中做一些小的改变，这样我们就有更好的机会为最小化成本函数的权值找到真正的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backPropagate(self, targets, N):\n",
    "    \"\"\"\n",
    "    :param targets: y values\n",
    "    :param N: learning rate\n",
    "    :return: updated weights and current error\n",
    "    \"\"\"\n",
    "    if len(targets) != self.output:\n",
    "        raise ValueError('Wrong number of targets you silly goose!')\n",
    "    # calculate error terms for output\n",
    "    # the delta tell you which direction to change the weights\n",
    "    output_deltas = [0.0] * self.output\n",
    "    for k in range(self.output):\n",
    "        error = -(targets[k] - self.ao[k])\n",
    "        output_deltas[k] = dsigmoid(self.ao[k]) * error\n",
    "    # calculate error terms for hidden\n",
    "    # delta tells you which direction to change the weights\n",
    "    hidden_deltas = [0.0] * self.hidden\n",
    "    for j in range(self.hidden):\n",
    "        error = 0.0\n",
    "        for k in range(self.output):\n",
    "            error += output_deltas[k] * self.wo[j][k]\n",
    "        hidden_deltas[j] = dsigmoid(self.ah[j]) * error\n",
    "    # update the weights connecting hidden to output\n",
    "    for j in range(self.hidden):\n",
    "        for k in range(self.output):\n",
    "            change = output_deltas[k] * self.ah[j]\n",
    "            self.wo[j][k] -= N * change + self.co[j][k]\n",
    "            self.co[j][k] = change\n",
    "    # update the weights connecting input to hidden\n",
    "    for i in range(self.input):\n",
    "        for j in range(self.hidden):\n",
    "            change = hidden_deltas[j] * self.ai[i]\n",
    "            self.wi[i][j] -= N * change + self.ci[i][j]\n",
    "            self.ci[i][j] = change\n",
    "    # calculate error\n",
    "    error = 0.0\n",
    "    for k in range(len(targets)):\n",
    "        error += 0.5 * (targets[k] - self.ao[k]) ** 2\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好的，让我们把它们链接在一起，创建训练和预测功能。训练网络的步骤是非常直接和直观的。我们首先调用“``前馈``”函数，它给出我们初始化的随机权值的输出。然后，我们调用反向传播算法来调整和更新权值，以做出更好的预测。然后再调用前馈函数，但这一次它使用了更新后的权值，预测结果略好一些。我们将这个循环保持在一个预先确定的迭代数量中，在此期间，我们应该看到错误下降到接近0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, patterns, iterations = 3000, N = 0.0002):\n",
    "    # N: learning rate\n",
    "    for i in range(iterations):\n",
    "        error = 0.0\n",
    "        for p in patterns:\n",
    "            inputs = p[0]\n",
    "            targets = p[1]\n",
    "            self.feedForward(inputs)\n",
    "            error = self.backPropagate(targets, N)\n",
    "        if i % 500 == 0:\n",
    "            print('error %-.5f' % error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，对于预测操作。我们只是简单地调用前馈函数，它将返回输出层的激活。记住，每一层的激活是前一层输出的线性组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "    \"\"\"\n",
    "    return list of predictions after training algorithm\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for p in X:\n",
    "        predictions.append(self.feedForward(p))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "[[0.0, 0.0, 0.6875, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 1.0, 1.0, 1.0, 0.8125, 0.0, 0.0, 0.0, 0.1875, 1.0, 0.75, 0.625, 0.875, 0.0, 0.0, 0.0, 0.0625, 1.0, 0.0625, 0.75, 0.9375, 0.0, 0.0, 0.0, 0.0, 0.8125, 1.0, 0.5625, 0.9375, 0.125, 0.0, 0.0, 0.0, 0.0, 0.1875, 0.0, 0.5625, 0.6875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5625, 0.9375, 0.25, 0.0, 0.0, 0.0, 0.5625, 0.75, 0.8125, 0.1875, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n",
      "error 463.71651\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-085496d521b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m     \u001b[0mdemo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-085496d521b8>\u001b[0m in \u001b[0;36mdemo\u001b[1;34m()\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[0mNN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP_NeuralNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrate_decay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m     \u001b[0mNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[0mNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-085496d521b8>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, patterns)\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m                 \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeedForward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m                 \u001b[0merror\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackPropagate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merrorfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-085496d521b8>\u001b[0m in \u001b[0;36mfeedForward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[0msum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m                 \u001b[0msum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mai\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mah\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "np.seterr(all = 'ignore')\n",
    "\n",
    "# sigmoid transfer function\n",
    "# IMPORTANT: when using the logit (sigmoid) transfer function for the output layer make sure y values are scaled from 0 to 1\n",
    "# if you use the tanh for the output then you should scale between -1 and 1\n",
    "# we will use sigmoid for the output layer and tanh for the hidden layer\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# derivative of sigmoid\n",
    "def dsigmoid(y):\n",
    "    return y * (1.0 - y)\n",
    "\n",
    "# using tanh over logistic sigmoid is recommended   \n",
    "def tanh(x):\n",
    "    return math.tanh(x)\n",
    "    \n",
    "# derivative for tanh sigmoid\n",
    "def dtanh(y):\n",
    "    return 1 - y*y\n",
    "\n",
    "class MLP_NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    Basic MultiLayer Perceptron (MLP) network, adapted and from the book 'Programming Collective Intelligence' (http://shop.oreilly.com/product/9780596529321.do)\n",
    "    Consists of three layers: input, hidden and output. The sizes of input and output must match data\n",
    "    the size of hidden is user defined when initializing the network.\n",
    "    The algorithm has been generalized to be used on any dataset.\n",
    "    As long as the data is in this format: [[[x1, x2, x3, ..., xn], [y1, y2, ..., yn]],\n",
    "                                           [[[x1, x2, x3, ..., xn], [y1, y2, ..., yn]],\n",
    "                                           ...\n",
    "                                           [[[x1, x2, x3, ..., xn], [y1, y2, ..., yn]]]\n",
    "    An example is provided below with the digit recognition dataset provided by sklearn\n",
    "    Fully pypy compatible.\n",
    "    \"\"\"\n",
    "    def __init__(self, input, hidden, output, iterations, learning_rate, momentum, rate_decay):\n",
    "        \"\"\"\n",
    "        :param input: number of input neurons\n",
    "        :param hidden: number of hidden neurons\n",
    "        :param output: number of output neurons\n",
    "        \"\"\"\n",
    "        # initialize parameters\n",
    "        self.iterations = iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.rate_decay = rate_decay\n",
    "        \n",
    "        # initialize arrays\n",
    "        self.input = input + 1 # add 1 for bias node\n",
    "        self.hidden = hidden\n",
    "        self.output = output\n",
    "\n",
    "        # set up array of 1s for activations\n",
    "        self.ai = [1.0] * self.input\n",
    "        self.ah = [1.0] * self.hidden\n",
    "        self.ao = [1.0] * self.output\n",
    "\n",
    "        # create randomized weights\n",
    "        # use scheme from 'efficient backprop to initialize weights\n",
    "        input_range = 1.0 / self.input ** (1/2)\n",
    "        output_range = 1.0 / self.hidden ** (1/2)\n",
    "        self.wi = np.random.normal(loc = 0, scale = input_range, size = (self.input, self.hidden))\n",
    "        self.wo = np.random.normal(loc = 0, scale = output_range, size = (self.hidden, self.output))\n",
    "        \n",
    "        # create arrays of 0 for changes\n",
    "        # this is essentially an array of temporary values that gets updated at each iteration\n",
    "        # based on how much the weights need to change in the following iteration\n",
    "        self.ci = np.zeros((self.input, self.hidden))\n",
    "        self.co = np.zeros((self.hidden, self.output))\n",
    "\n",
    "    def feedForward(self, inputs):\n",
    "        \"\"\"\n",
    "        The feedforward algorithm loops over all the nodes in the hidden layer and\n",
    "        adds together all the outputs from the input layer * their weights\n",
    "        the output of each node is the sigmoid function of the sum of all inputs\n",
    "        which is then passed on to the next layer.\n",
    "        :param inputs: input data\n",
    "        :return: updated activation output vector\n",
    "        \"\"\"\n",
    "        if len(inputs) != self.input-1:\n",
    "            raise ValueError('Wrong number of inputs you silly goose!')\n",
    "\n",
    "        # input activations\n",
    "        for i in range(self.input -1): # -1 is to avoid the bias\n",
    "            self.ai[i] = inputs[i]\n",
    "\n",
    "        # hidden activations\n",
    "        for j in range(self.hidden):\n",
    "            sum = 0.0\n",
    "            for i in range(self.input):\n",
    "                sum += self.ai[i] * self.wi[i][j]\n",
    "            self.ah[j] = tanh(sum)\n",
    "\n",
    "        # output activations\n",
    "        for k in range(self.output):\n",
    "            sum = 0.0\n",
    "            for j in range(self.hidden):\n",
    "                sum += self.ah[j] * self.wo[j][k]\n",
    "            self.ao[k] = sigmoid(sum)\n",
    "\n",
    "        return self.ao[:]\n",
    "\n",
    "    def backPropagate(self, targets):\n",
    "        \"\"\"\n",
    "        For the output layer\n",
    "        1. Calculates the difference between output value and target value\n",
    "        2. Get the derivative (slope) of the sigmoid function in order to determine how much the weights need to change\n",
    "        3. update the weights for every node based on the learning rate and sig derivative\n",
    "\n",
    "        For the hidden layer\n",
    "        1. calculate the sum of the strength of each output link multiplied by how much the target node has to change\n",
    "        2. get derivative to determine how much weights need to change\n",
    "        3. change the weights based on learning rate and derivative\n",
    "        :param targets: y values\n",
    "        :param N: learning rate\n",
    "        :return: updated weights\n",
    "        \"\"\"\n",
    "        if len(targets) != self.output:\n",
    "            raise ValueError('Wrong number of targets you silly goose!')\n",
    "\n",
    "        # calculate error terms for output\n",
    "        # the delta tell you which direction to change the weights\n",
    "        output_deltas = [0.0] * self.output\n",
    "        for k in range(self.output):\n",
    "            error = -(targets[k] - self.ao[k])\n",
    "            output_deltas[k] = dsigmoid(self.ao[k]) * error\n",
    "\n",
    "        # calculate error terms for hidden\n",
    "        # delta tells you which direction to change the weights\n",
    "        hidden_deltas = [0.0] * self.hidden\n",
    "        for j in range(self.hidden):\n",
    "            error = 0.0\n",
    "            for k in range(self.output):\n",
    "                error += output_deltas[k] * self.wo[j][k]\n",
    "            hidden_deltas[j] = dtanh(self.ah[j]) * error\n",
    "\n",
    "        # update the weights connecting hidden to output\n",
    "        for j in range(self.hidden):\n",
    "            for k in range(self.output):\n",
    "                change = output_deltas[k] * self.ah[j]\n",
    "                self.wo[j][k] -= self.learning_rate * change + self.co[j][k] * self.momentum\n",
    "                self.co[j][k] = change\n",
    "\n",
    "        # update the weights connecting input to hidden\n",
    "        for i in range(self.input):\n",
    "            for j in range(self.hidden):\n",
    "                change = hidden_deltas[j] * self.ai[i]\n",
    "                self.wi[i][j] -= self.learning_rate * change + self.ci[i][j] * self.momentum\n",
    "                self.ci[i][j] = change\n",
    "\n",
    "        # calculate error\n",
    "        error = 0.0\n",
    "        for k in range(len(targets)):\n",
    "            error += 0.5 * (targets[k] - self.ao[k]) ** 2\n",
    "        return error\n",
    "\n",
    "    def test(self, patterns):\n",
    "        \"\"\"\n",
    "        Currently this will print out the targets next to the predictions.\n",
    "        Not useful for actual ML, just for visual inspection.\n",
    "        \"\"\"\n",
    "        for p in patterns:\n",
    "            print(p[1], '->', self.feedForward(p[0]))\n",
    "\n",
    "    def train(self, patterns):\n",
    "        # N: learning rate\n",
    "        for i in range(self.iterations):\n",
    "            error = 0.0\n",
    "            random.shuffle(patterns)\n",
    "            for p in patterns:\n",
    "                inputs = p[0]\n",
    "                targets = p[1]\n",
    "                self.feedForward(inputs)\n",
    "                error += self.backPropagate(targets)\n",
    "            with open('error.txt', 'a') as errorfile:\n",
    "                errorfile.write(str(error) + '\\n')\n",
    "                errorfile.close()\n",
    "            if i % 10 == 0:\n",
    "                print('error %-.5f' % error)\n",
    "            # learning rate decay\n",
    "            self.learning_rate = self.learning_rate * (self.learning_rate / (self.learning_rate + (self.learning_rate * self.rate_decay)))\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        return list of predictions after training algorithm\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for p in X:\n",
    "            predictions.append(self.feedForward(p))\n",
    "        return predictions\n",
    "\n",
    "def demo():\n",
    "    \"\"\"\n",
    "    run NN demo on the digit recognition dataset from sklearn\n",
    "    \"\"\"\n",
    "    def load_data():\n",
    "        data = np.loadtxt('sklearn_digits.csv', delimiter = ',')\n",
    "\n",
    "        # first ten values are the one hot encoded y (target) values\n",
    "        y = data[:,0:10]\n",
    "        #y[y == 0] = -1 # if you are using a tanh transfer function make the 0 into -1\n",
    "        #y[y == 1] = .90 # try values that won't saturate tanh\n",
    "        \n",
    "        data = data[:,10:] # x data\n",
    "        #data = data - data.mean(axis = 1)\n",
    "        data -= data.min() # scale the data so values are between 0 and 1\n",
    "        data /= data.max() # scale\n",
    "        \n",
    "        out = []\n",
    "        print(data.shape)\n",
    "\n",
    "        # populate the tuple list with the data\n",
    "        for i in range(data.shape[0]):\n",
    "            fart = list((data[i,:].tolist(), y[i].tolist())) # don't mind this variable name\n",
    "            out.append(fart)\n",
    "\n",
    "        return out\n",
    "\n",
    "    X = load_data()\n",
    "\n",
    "    print(X[9]) # make sure the data looks right\n",
    "\n",
    "    NN = MLP_NeuralNetwork(64, 100, 10, iterations = 50, learning_rate = 0.5, momentum = 0.5, rate_decay = 0.01)\n",
    "\n",
    "    NN.train(X)\n",
    "\n",
    "    NN.test(X)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    demo()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
